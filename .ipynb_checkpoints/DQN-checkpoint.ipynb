{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.insert(0, 'Resources/MagicCube/code/')\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "from bitstring import BitArray\n",
    "import math\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from cube import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 3 #cube size\n",
    "\n",
    "#Q-learning parameters\n",
    "r = 0.15\n",
    "gamma = 1 / (1 + r) #discount of the model\n",
    "C = 1.\n",
    "epsilon = 0.05\n",
    "beta = 3./4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_actions(N): #rotate by +90° / by -90° \n",
    "    actions = []\n",
    "    c = Cube(N)\n",
    "    for face_name in [\"F\",\"U\",\"R\"]: #the list is in the end ['U','D','F']\n",
    "        for layer in range(c.N):\n",
    "            for times in [1,-1]:\n",
    "                actions.append([face_name,layer,times])\n",
    "    return actions\n",
    "\n",
    "# def reward_cube(c):\n",
    "#     edges = computeEdges(c)\n",
    "#     corners = computeCorners(c)\n",
    "#     ncf = numCompleteFaces(c)\n",
    "#     nce = numCompleteEdges(c,edges)\n",
    "#     ncc = numCompleteCorners(c,corners)\n",
    "    \n",
    "# #     return (-1 + 10*ncf + 2*nce + 3*ncc + 100*(ncf == 6))/700\n",
    "#     return (ncf == 6)\n",
    "\n",
    "def reward_cube(c):\n",
    "    ncf = numCompleteFaces(c)\n",
    "    return (-1 + entropy(c) + 100*(ncf == 6))/100\n",
    "\n",
    "def entropy(c):\n",
    "    ent = 0\n",
    "    for f in range(6):\n",
    "        pi = len(np.unique(c.stickers[f]))\n",
    "        ent -= pi*np.log(pi)       \n",
    "    return ent\n",
    "        \n",
    "\n",
    "def state_cube(c):\n",
    "    #determining the new state\n",
    "    edges = computeEdges(c)\n",
    "    corners = computeCorners(c)\n",
    "    edges_state = []\n",
    "    corners_state = []\n",
    "    faces_state = []\n",
    "#     for e in edges:\n",
    "#         edges_state.append(e.isDone(c))\n",
    "    for corner in corners:\n",
    "        corners_state.append(corner.isDone(c))\n",
    "    nFaces = 6\n",
    "    for f in range(nFaces):\n",
    "        faces_state.append(np.sum(c.stickers[f] != c.stickers[f,0,0]) == 0)\n",
    "#     #conversion from binary list to int\n",
    "#     e = BitArray(edges_state).uint\n",
    "    c = BitArray(corners_state).uint\n",
    "    f = BitArray(faces_state).uint\n",
    "#     ncf = numCompleteFaces(c)\n",
    "#     nce = numCompleteEdges(c,edges)\n",
    "#     ncc = numCompleteCorners(c,corners)\n",
    "#     return ncf,nce,ncc\n",
    "    return c,f\n",
    "\n",
    "def test_function_state_cube():\n",
    "    c = Cube(3)\n",
    "    print(state_cube(c))\n",
    "    c.randomize(1)\n",
    "    print(state_cube(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['F', 0, 1],\n",
       " ['F', 0, -1],\n",
       " ['F', 1, 1],\n",
       " ['F', 1, -1],\n",
       " ['F', 2, 1],\n",
       " ['F', 2, -1],\n",
       " ['U', 0, 1],\n",
       " ['U', 0, -1],\n",
       " ['U', 1, 1],\n",
       " ['U', 1, -1],\n",
       " ['U', 2, 1],\n",
       " ['U', 2, -1],\n",
       " ['R', 0, 1],\n",
       " ['R', 0, -1],\n",
       " ['R', 1, 1],\n",
       " ['R', 1, -1],\n",
       " ['R', 2, 1],\n",
       " ['R', 2, -1]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = all_actions(N) #rotate by +90° / by -90° \n",
    "nb_actions = len(actions)\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class network():\n",
    "    \n",
    "    def __init__(self,W1,W2):\n",
    "        \n",
    "        self.W1 = W1\n",
    "        self.W2 = W2\n",
    "        \n",
    "        self.Q1 = tf.matmul(x/6,self.W1)# + b1\n",
    "        self.Qs1 = tf.nn.tanh(self.Q1)\n",
    "        self.Q2 = tf.matmul(self.Qs1,self.W2)#tf.nn.relu(tf.matmul(Qs1,W2))# + b2)\n",
    "        \n",
    "        self.sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "         \n",
    "        self.network_params = tf.trainable_variables()\n",
    "        self.tau = 1.\n",
    "    \n",
    "    \n",
    "    \n",
    "    def update_target_network(self, trainNet):\n",
    "        \n",
    "        self.update_target_network_params = \\\n",
    "            [self.network_params[i].assign(tf.mul(trainNet.network_params[i], self.tau) + \\\n",
    "                tf.mul(self.network_params[i], 1. - self.tau))\n",
    "                for i in range(len(self.network_params))]\n",
    "        \n",
    "        self.sess.run(self.update_target_network_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_init=Cube(3)\n",
    "\n",
    "# resume = sys.argv[1] == \"True\"\n",
    "resume = True\n",
    "\n",
    "# sess = tf.InteractiveSession()\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True)) \n",
    "with tf.device(\"/cpu:0\"):\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[None,6*c_init.N**2])\n",
    "    Q_ = tf.placeholder(tf.float32, shape=[None,nb_actions])\n",
    "    n_layers = 1\n",
    "\n",
    "    if not resume:\n",
    "        middle_layer = 500\n",
    "        W1 = tf.Variable(tf.random_normal([6*c_init.N**2,middle_layer], stddev=1e-2))\n",
    "        b1 = tf.Variable(tf.random_normal([middle_layer], stddev=1e-2))\n",
    "\n",
    "        W2 = tf.Variable(tf.random_normal([middle_layer,nb_actions], stddev=1e-2))\n",
    "        b2 = tf.Variable(tf.random_normal([nb_actions], stddev=1e-2)) \n",
    "\n",
    "        for i in range(n_layers):\n",
    "            globals()['Wm_%s'%i] = tf.Variable(tf.random_normal([middle_layer,middle_layer], stddev=1e-2))\n",
    "            globals()['bm_%s'%i]  = tf.Variable(tf.random_normal([middle_layer], stddev=1e-2)) \n",
    "        \n",
    "    else:\n",
    "        load = pickle.load(open('save.p', 'rb'))\n",
    "        W1 = tf.Variable(load[0])\n",
    "        W2 = tf.Variable(load[1])\n",
    "        b1 = tf.Variable(load[2])\n",
    "        b2 = tf.Variable(load[3])\n",
    "        n_layers = len(load[4])\n",
    "        for i in range(n_layers):\n",
    "            globals()['Wm_%s'%i] = tf.Variable(load[4][i])\n",
    "            globals()['bm_%s'%i]  = tf.Variable(load[5][i])\n",
    "\n",
    "\n",
    "    Q1 = tf.nn.tanh(tf.matmul(x/6,W1) + b1)\n",
    "    \n",
    "    Qm_0 = tf.nn.tanh(tf.matmul(Q1,Wm_0) + bm_0)\n",
    "    \n",
    "    for i in range(1,n_layers):\n",
    "        \n",
    "        globals()['Qm_%s'%i] = tf.nn.tanh(tf.matmul(globals()['Qm_%s'%(i-1)],globals()['Wm_%s'%i]) + globals()['bm_%s'%i])\n",
    "        \n",
    "        \n",
    "    Q2 = tf.matmul(globals()['Qm_%s'%(n_layers-1)],W2) + b2 #tf.nn.relu(tf.matmul(Qs1,W2))# + b2)\n",
    "    # Qs = tf.matmul(Q2,act)\n",
    "    Qs = Q2\n",
    "\n",
    "    \n",
    "    loss_function = tf.reduce_mean(tf.square(tf.sub(Q_,Qs)))\n",
    "\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def DQN(c_init,Tmax,nb_episodes, n_moves):\n",
    "    \n",
    "    done = 0\n",
    "    lActions = np.zeros(18)\n",
    "    print(\"moves\",\"\\t\",\"ep.\",\"\\t\",\"Loss Function\",\"\\t\",\"Min Q\",\"\\t\\t\", \"Reward\", \"\", \"NB.\",\"\\t\", \"Prcent.\")\n",
    "    \n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        train_step = tf.train.RMSPropOptimizer(0.002).minimize(loss_function)\n",
    "        sess.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))\n",
    "        \n",
    "    mineps = 10./100\n",
    "    epssteps = 1e6\n",
    "    def eps(episode):\n",
    "        return 1-(1-mineps)*min(1,episode/(epssteps))\n",
    "    \n",
    "    percentDone = np.array([0])\n",
    "    percentDoneMean = np.array([0])\n",
    "\n",
    "    episode = 1\n",
    "    \n",
    "    tries = 1\n",
    "    \n",
    "    dones = np.empty([0])\n",
    "    \n",
    "    D = []\n",
    "    \n",
    "    while percentDoneMean[-1] < .95 and episode < nb_episodes:  \n",
    "        \n",
    "        episode += 1\n",
    "        \n",
    "        s = copy.deepcopy(c_init)\n",
    "        s.randomize(n_moves) #we randomize n_moves times in order to have a \"well mixed\" cube\n",
    "        #s.move(\"R\",2,-1)\n",
    "        cum_reward = []\n",
    "        \n",
    "        tries += 1\n",
    "        done = 0\n",
    "            \n",
    "        for i in range(Tmax):\n",
    "            \n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            S = copy.deepcopy(np.reshape(s.stickers,(1, 54)))\n",
    "            Qout = sess.run(Q2,feed_dict={x:S})\n",
    "            if(rd.random() > eps(episode)):\n",
    "                a = np.argmax(Qout)\n",
    "            else:\n",
    "                a = rd.randint(0,nb_actions-1)\n",
    "\n",
    "            lActions[a] += 1\n",
    "            \n",
    "            #print(actions[a])\n",
    "            #print(Qout)\n",
    "            #Get new state and reward from environment\n",
    "            f,l,d = actions[a]\n",
    "            #print(actions[a])\n",
    "            s.move(f,l,d)            \n",
    "            r = reward_cube(s)\n",
    "            cum_reward.append(r)\n",
    "            D.append(copy.deepcopy([S, a, r, np.reshape(s.stickers,(1, 54)) , numCompleteFaces(s)]))\n",
    "            \n",
    "            #print(S)\n",
    "            #print(np.reshape(s.stickers,(1, 54)))\n",
    "            \n",
    "            #Obtain the Q' values by feeding the new state through our network\n",
    "#==============================================================================\n",
    "#            Qprime = sess.run(Q2,feed_dict={x:np.reshape(s.stickers,(1, 54))})\n",
    "             #Obtain maxQ' and set our target value for chosen action.\n",
    "#            maxQprime = np.max(Qprime)\n",
    "#            targetQ = Qout\n",
    "#            targetQ[0,a] = r + gamma*maxQprime\n",
    "             #Train our network using target and predicted Q values\n",
    "#                \n",
    "#             sess.run(train_step,feed_dict={Q_: targetQ, x: S})\n",
    "#==============================================================================\n",
    "            \n",
    "            #print(targetQ)\n",
    "\n",
    "            \n",
    "# ============================================================================== \n",
    "#                           EXPERIENCE REPLAY      \n",
    "# ==============================================================================\n",
    "    \n",
    "    \n",
    "            if len(D) == 16: # BATCH SIZE by Guillaume Lample\n",
    "                batch = copy.deepcopy(np.array(D))\n",
    "                random.shuffle(batch)\n",
    "                tts = np.empty([0,nb_actions])\n",
    "                \n",
    "                for i in range(len(batch)):\n",
    "                  \n",
    "                    faces_done = batch[i][-1]\n",
    "                    Qprime = sess.run(Q2,feed_dict={x:batch[i][-2]})\n",
    "                    maxQprime = np.max(Qprime)\n",
    "                    \n",
    "                    tt = sess.run(Q2,feed_dict={x:batch[i][0]})\n",
    "                    if faces_done >= 6:\n",
    "                        tt[0,batch[i][1]] = batch[i][-3]\n",
    "                    else:\n",
    "                        tt[0,batch[i][1]] = batch[i][-3] + gamma*maxQprime\n",
    "                  \n",
    "                    tts = np.concatenate((tts,tt),0)\n",
    "                  \n",
    "                sess.run(train_step,feed_dict={Q_: tts, x: np.vstack(batch[:,0])})\n",
    "                \n",
    "                D = []\n",
    "    \n",
    "# ============================================================================== \n",
    "#                           \n",
    "# ==============================================================================\n",
    "\n",
    "            if numCompleteFaces(s) == 6:\n",
    "                done = 1\n",
    "                break\n",
    "            \n",
    "        dones = np.append(dones,done)\n",
    "            \n",
    "        \n",
    "            \n",
    "#        if episode%100 == 1:\n",
    "##             sess.run(loss_function,feed_dict={Q_: targetQ, x: S}),\"\\t\",\n",
    "#            print(n_moves,\"\\t\",episode,\"\\t\",min(sess.run(Q2,feed_dict={x:S})[0]),\"\\t\", round(np.mean(cum_reward[-1]),2), \"\\t\", np.sum(dones[-1000:]),\"\\t\", round(100*np.sum(dones[-1000:])/min(1000,tries),2))\n",
    "#    #             print(lActions)\n",
    "#            percentDone.append(100*np.sum(dones[-1000:])/min(1000,tries))\n",
    "            \n",
    "        if episode%1000 == 1:\n",
    "            #Evaluation of the policy\n",
    "#            print(\"Evaluation after training: actions maximizing the reward, no more random\")\n",
    "            dones_evaluation = np.empty([0])\n",
    "            N_iteration_evaluation = 100\n",
    "            for _ in range(N_iteration_evaluation):\n",
    "                done_evaluation = 0\n",
    "                s = copy.deepcopy(c_init)\n",
    "                while numCompleteFaces(s) == 6: #in order not to start with the solved cube\n",
    "                    s.randomize(n_moves) #we randomize n_moves times in order to have a \"well mixed\" cube\n",
    "                for i in range(Tmax):\n",
    "                    S = copy.deepcopy(np.reshape(s.stickers,(1, 54)))\n",
    "                    #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "                    Qout = sess.run(Q2,feed_dict={x:S})\n",
    "                    a = np.argmax(Qout)\n",
    "                    #Get new state and reward from environment\n",
    "                    f,l,d = actions[a]\n",
    "                    s.move(f,l,d)   \n",
    "                    if numCompleteFaces(s) == 6:\n",
    "                        done_evaluation = 1\n",
    "                        break\n",
    "                dones_evaluation = np.append(dones_evaluation,done_evaluation)        \n",
    "            print(n_moves,\"\\t\", episode, \"\\t\", np.round(percentDoneMean[-1],2), \"\\t\", round(np.mean(cum_reward[-1]),2),\"\\t\", np.sum(dones_evaluation), \"\\t\", round(100*np.mean(dones_evaluation),2))\n",
    "\n",
    "            percentDone = np.append(percentDone,np.mean(dones_evaluation))\n",
    "            percentDoneMean = np.append(percentDoneMean, np.mean(percentDone[-10:]))\n",
    "            \n",
    "            plt.clf()\n",
    "            plt.plot(100*percentDone, linewidth = 2, alpha = 0.5)\n",
    "            plt.plot(100*percentDoneMean, linewidth = 2)\n",
    "            plt.title(\"n_moves: \"+str(n_moves))\n",
    "            plt.xlabel(\"Nb_episodes (*1000)\")\n",
    "            plt.ylabel(\"Accuracy\")\n",
    "            plt.pause(0.0001)\n",
    "            \n",
    "        if episode%10000 == 1:\n",
    "            topickle = [sess.run(W1),sess.run(W2),sess.run(b1),sess.run(b2)]\n",
    "            topickle.append([sess.run(globals()['Wm_%s'%i]) for i in range(n_layers)])\n",
    "            topickle.append([sess.run(globals()['bm_%s'%i]) for i in range(n_layers)])\n",
    "            pickle.dump(topickle, open('save.p', 'wb'))\n",
    "            \n",
    "    plt.title(\"n_moves: \"+str(n_moves)+\" - \"+str(int(100*percentDoneMean[-1]))+\"%\")\n",
    "    plt.savefig(\"./learning_curves/n_moves_\"+str(n_moves)+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def longTrain(c_init,n_moves_max):\n",
    "\n",
    "    for i in range(1,1+n_moves_max):\n",
    "        print(\"==============================================================================\")\n",
    "        print(\"\\t\",i,\"Moves\",\"\\t\")\n",
    "        print(\"==============================================================================\")\n",
    "        DQN(c_init=c_init,Tmax=i,nb_episodes=1000000,n_moves = i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def success(n_max):\n",
    "\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        train_step = tf.train.RMSPropOptimizer(0.002).minimize(loss_function)\n",
    "        sess.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))\n",
    "\n",
    "    percentages = np.empty([0])\n",
    "\n",
    "    for n_moves in range(1,n_max):\n",
    "\n",
    "        dones_evaluation = np.empty([0])\n",
    "        N_iteration_evaluation = 100\n",
    "        for _ in range(N_iteration_evaluation):\n",
    "            done_evaluation = 0\n",
    "            s = copy.deepcopy(c_init)\n",
    "            while numCompleteFaces(s) == 6: #in order not to start with the solved cube\n",
    "                s.randomize(n_moves) #we randomize n_moves times in order to have a \"well mixed\" cube\n",
    "            for i in range(n_moves):\n",
    "                S = copy.deepcopy(np.reshape(s.stickers,(1, 54)))\n",
    "                #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "                Qout = sess.run(Q2,feed_dict={x:S})\n",
    "                a = np.argmax(Qout)\n",
    "                #Get new state and reward from environment\n",
    "                f,l,d = actions[a]\n",
    "                s.move(f,l,d)   \n",
    "                if numCompleteFaces(s) == 6:\n",
    "                    done_evaluation = 1\n",
    "                    break\n",
    "\n",
    "            dones_evaluation = np.append(dones_evaluation,done_evaluation)\n",
    "\n",
    "        percentages = np.append(percentages, np.mean(dones_evaluation))\n",
    "    \n",
    "    plt.plot(range(1,n_max), 100*percentages)\n",
    "    \n",
    "    return 100*percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05,  0.01,  0.03,  0.01,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "        0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAAFkCAYAAADFZ4k9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEkZMPiSJGXg/wMHNY7vVKDWrQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5db4343e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "success(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with tf.device(\"/gpu:0\"):\n",
    "#     longTrain(Cube(3),10)\n",
    "# #    DQN(c_init=Cube(3),Tmax=int(sys.argv[4]),nb_episodes=int(sys.argv[2]),n_moves = int(sys.argv[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# topickle = [sess.run(W1),sess.run(W2)]\n",
    "# pickle.dump(topickle, open('save.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init = tf.initialize_all_variables()\n",
    "\n",
    "# # Set learning parameters\n",
    "# y = .99\n",
    "# e = 0.1\n",
    "# num_episodes = 2000\n",
    "# #create lists to contain total rewards and steps per episode\n",
    "# jList = []\n",
    "# rList = []\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "#     for i in range(num_episodes):\n",
    "#         #Reset environment and get first new observation\n",
    "#         s = env.reset()\n",
    "#         rAll = 0\n",
    "#         d = False\n",
    "#         j = 0\n",
    "#         #The Q-Network\n",
    "#         while j < 99:\n",
    "#             j+=1\n",
    "#             #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "#             a,allQ = sess.run([predict,Qout],feed_dict={inputs1:np.identity(16)[s:s+1]})\n",
    "#             if np.random.rand(1) < e:\n",
    "#                 a[0] = env.action_space.sample()\n",
    "#             #Get new state and reward from environment\n",
    "#             s1,r,d,_ = env.step(a[0])\n",
    "#             #Obtain the Q' values by feeding the new state through our network\n",
    "#             Q1 = sess.run(Qout,feed_dict={inputs1:np.identity(16)[s1:s1+1]})\n",
    "#             #Obtain maxQ' and set our target value for chosen action.\n",
    "#             maxQ1 = np.max(Q1)\n",
    "#             targetQ = allQ\n",
    "#             targetQ[0,a[0]] = r + y*maxQ1\n",
    "#             #Train our network using target and predicted Q values\n",
    "#             _,W1 = sess.run([updateModel,W],feed_dict={inputs1:np.identity(16)[s:s+1],nextQ:targetQ})\n",
    "#             rAll += r\n",
    "#             s = s1\n",
    "#             if d == True:\n",
    "#                 #Reduce chance of random action as we train the model.\n",
    "#                 e = 1./((i/50) + 10)\n",
    "#                 break\n",
    "#         jList.append(j)\n",
    "#         rList.append(rAll)\n",
    "# print \"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Taken from https://gist.github.com/awjuliani/fffe41519166ee41a6bd5f5ce8ae2630#file-double-dueling-dqn-tutorial-ipynb\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     if load_model == True:\n",
    "#         print 'Loading Model...'\n",
    "#         ckpt = tf.train.get_checkpoint_state(path)\n",
    "#         saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "#     sess.run(init)\n",
    "#     updateTarget(targetOps,sess) #Set the target network to be equal to the primary network.\n",
    "#     for i in range(num_episodes):\n",
    "#         episodeBuffer = experience_buffer()\n",
    "#         #Reset environment and get first new observation\n",
    "#         s = env.reset()\n",
    "#         s = processState(s)\n",
    "#         d = False\n",
    "#         rAll = 0\n",
    "#         j = 0\n",
    "#         #The Q-Network\n",
    "#         while j < max_epLength: #If the agent takes longer than 200 moves to reach either of the blocks, end the trial.\n",
    "#             j+=1\n",
    "#             #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "#             if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "#                 a = np.random.randint(0,4)\n",
    "#             else:\n",
    "#                 a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "#             s1,r,d = env.step(a)\n",
    "#             s1 = processState(s1)\n",
    "#             total_steps += 1\n",
    "#             episodeBuffer.add(np.reshape(np.array([s,a,r,s1,d]),[1,5])) #Save the experience to our episode buffer.\n",
    "            \n",
    "#             if total_steps > pre_train_steps:\n",
    "#                 if e > endE:\n",
    "#                     e -= stepDrop\n",
    "                \n",
    "#                 if total_steps % (update_freq) == 0:\n",
    "#                     trainBatch = myBuffer.sample(batch_size) #Get a random batch of experiences.\n",
    "#                     #Below we perform the Double-DQN update to the target Q-values\n",
    "#                     Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "#                     Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "#                     end_multiplier = -(trainBatch[:,4] - 1)\n",
    "#                     doubleQ = Q2[range(batch_size),Q1]\n",
    "#                     targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "#                     #Update the network with our target values.\n",
    "#                     _ = sess.run(mainQN.updateModel, \\\n",
    "#                         feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n",
    "                    \n",
    "#                     updateTarget(targetOps,sess) #Set the target network to be equal to the primary network.\n",
    "#             rAll += r\n",
    "#             s = s1\n",
    "            \n",
    "#             if d == True:\n",
    "\n",
    "#                 break\n",
    "        \n",
    "#         #Get all experiences from this episode and discount their rewards.\n",
    "#         myBuffer.add(episodeBuffer.buffer)\n",
    "#         jList.append(j)\n",
    "#         rList.append(rAll)\n",
    "#         #Periodically save the model. \n",
    "#         if i % 1000 == 0:\n",
    "#             saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "#             print \"Saved Model\"\n",
    "#         if len(rList) % 10 == 0:\n",
    "#             print total_steps,np.mean(rList[-10:]), e\n",
    "#     saver.save(sess,path+'/model-'+str(i)+'.cptk')\n",
    "# print \"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### BACKUP DON'T TOUCH PLEASE ###\n",
    "\n",
    "\n",
    "# def DQN(c_init,Tmax,nb_episodes, n_moves):\n",
    "    \n",
    "#     done = 0\n",
    "#     lActions = np.zeros(18)\n",
    "#     print(\"ep.\",\"\\t\",\"Loss Function\",\"\\t\",\"Min Q\",\"\\t\\t\", \"Reward\", \"\", \"NB.\",\"\\t\", \"Prcent.\")\n",
    "#     def eps(episode):\n",
    "#         return min(1,max(.05,100/(1+episode)))\n",
    "    \n",
    "#     episode = 1\n",
    "#     tries = 0\n",
    "    \n",
    "#     for k in range(nb_episodes):  \n",
    "        \n",
    "#         s = copy.deepcopy(c_init)\n",
    "#         s.randomize(n_moves) #we randomize n_moves times in order to have a \"well mixed\" cube\n",
    "#         cum_reward = []\n",
    "        \n",
    "#         tries += 1\n",
    "        \n",
    "#         for i in range(Tmax):\n",
    "            \n",
    "#             #transition \n",
    "#             S = np.reshape(s.stickers,(1, 54))\n",
    "#             if(rd.random() > eps(episode)):\n",
    "#                 a = np.argmax(Q2.eval({x:S}))\n",
    "#             else:\n",
    "#                 a = rd.randint(0,nb_actions-1)\n",
    "\n",
    "#             lActions[a] += 1\n",
    "#             f,l,d = actions[a]\n",
    "#             s.move(f,l,d)            \n",
    "#             R = reward_cube(s)\n",
    "#             cum_reward.append(R)\n",
    "#             D.append([S, a, R, np.reshape(s.stickers,(1, 54))])\n",
    "            \n",
    "#             if numCompleteFaces(s) == 6:\n",
    "#                 done += 1\n",
    "                \n",
    "#                 Dshuf = D\n",
    "#                 random.shuffle(Dshuf)\n",
    "#                 batch = np.array(Dshuf[:10])\n",
    "\n",
    "#                 tts = np.empty([0,1])\n",
    "\n",
    "#                 for i in range(len(batch)):\n",
    "\n",
    "#                     faces_done = np.sum([np.sum([batch[i][-1][0][f*9+j] != batch[i][-1][0][f*9] for j in range(9)]) == 0 for f in range(6)])\n",
    "\n",
    "#                     if faces_done == 6:\n",
    "#                         tt = batch[i][-2]\n",
    "#                     else:\n",
    "#                         tt = batch[i][-2] + gamma*max(Q2.eval({x:batch[i][-1]})[0])\n",
    "\n",
    "#                     tts = np.append(tts,tt)\n",
    "\n",
    "#                 tts = np.reshape(tts,(-1, 1))\n",
    "\n",
    "#                 action = np.reshape([[i == batch[j,1] for i in range(nb_actions)] for j in range(len(batch))], (-1,len(batch)))\n",
    "#                 train_step.run(feed_dict={Q_: tts, x: batch[:,0][0], act: action})\n",
    "                \n",
    "        \n",
    "# #         if episode%10 == 1:\n",
    "#                 print(episode,\"\\t\",loss_function.eval({Q_: tts, x:batch[:,0][0], act: action}),\"\\t\",min(Q2.eval({x:batch[:,0][0]})[0]),\"\\t\", round(cum_reward[-1],2), \"\\t\", done,\"\\t\", round(100*done/tries,2))\n",
    "#         #             print(lActions)\n",
    "#                 print(np.var(Q2.eval({x:batch[:,0][0]})[0]))\n",
    "            \n",
    "#                 break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
